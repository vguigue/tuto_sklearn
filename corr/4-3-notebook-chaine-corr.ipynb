{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f467d3",
   "metadata": {},
   "source": [
    "# C. Construction d'une chaine de traitements en scikit-learn\n",
    "\n",
    "## C.1. Cas d'usage: sélection de caractéristique\n",
    "\n",
    "1. Construire des données bruitées\n",
    "2. Construire un sélecteur de variables pertinentes \n",
    "    * Afin de prolonger le TP précedent, on choisit de construire un sélecteur basé sur la corrélation... En utilisant l'héritage pour une parfaite intégration dans la chaine\n",
    "3. L'intégrer dans la chaine (`Pipeline`)\n",
    "4. Vérifier:\n",
    "    * que la chaine est apprenable\n",
    "    * évaluable\n",
    "    * que les éléments de la chaine ont des dimensions raisonnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# données + dimensions inutiles\n",
    "centers = [[-2.0, -2.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 1.5]\n",
    "X, y = make_blobs(n_samples=100, centers=centers, cluster_std=clusters_std,  n_features=2,   random_state=0) # 100 pts, 2classes, 2dim \n",
    "\n",
    "# ajout de bruit pour rendre le problème plus difficile\n",
    "ndim_noise = 20\n",
    "Noise = np.random.randn(len(X), ndim_noise)*clusters_std[0]\n",
    "Xn = np.concatenate((X,Noise), axis=1)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b40646",
   "metadata": {},
   "source": [
    "Pour la procédure de sélection de variable, je fais un truc marrant en programmation objet (mais peu interressant du point de vue machine learning)\n",
    "1. Construction d'une machine à score = corrélation des caractéristiques avec la cible (très naïf)\n",
    "1. Intégration de cet outil perso dans une chaine de traitement sklearn: un sélecteur forward (ça, c'est marrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4588603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction d'un estimateur sklearn calculant la corrélation des différentes features avec la cible\n",
    "# => fit ne fait rien\n",
    "# => score renvoie la somme des correlations des variables retenues\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Correl(BaseEstimator):\n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    def score(self,X,y):\n",
    "        return np.sum(X.T@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca73a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection de caractéristiques\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "estimator = Correl()\n",
    "selector = SequentialFeatureSelector(estimator, n_features_to_select=4) # j'utilise mon outil dans le selecteur sklearn\n",
    "                                                                        # je lui demande de prendre les 4 meilleures caractéristiques\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "print(selector.get_support())\n",
    "\n",
    "# il est ensuite possible de filtrer les données:\n",
    "Xnew = selector.transform(X_train) # ouf! les deux premières sont sélectionnées (+ 2 autres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dd637",
   "metadata": {},
   "source": [
    "Construction du pipeline: sélection des variables + invocation du classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a333e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la chaine (pipeline)\n",
    "from sklearn.pipeline import Pipeline\n",
    "classif = svm.SVC(kernel = 'linear')\n",
    "# liste de tuples: titre + objet \n",
    "pipe = Pipeline([('sel. var',selector),('classif',classif)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vérification de l'usage et des propriétés du nouvel objet\n",
    "pipe.fit(X_train,y_train)           # 1. Apprentissage\n",
    "yhat = pipe.predict(X_test)         # 2. inférence\n",
    "tx = accuracy_score(yhat, y_test)   # 3. Evaluation (même si pipe n'entre pas directement en jeu)\n",
    "\n",
    "print(\"taux de bonne classif: \", tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des dimensions du classifieur\n",
    "print(\"Dimension du classifieur: \", classif.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5033907",
   "metadata": {},
   "source": [
    "## C.2. Même exercice avec une PCA & un arbre de décision\n",
    "\n",
    "**Exercice :** Construire une chaine composée de \n",
    "* Une projection PCA sur 3 axes \n",
    "* Un arbre de décision\n",
    "* Calculer la performance de cette chaine et afficher l'arbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "\n",
    "### <CORRECTION> ###\n",
    "\n",
    "selvar = PCA(n_components=3)\n",
    "classif = DecisionTreeClassifier()\n",
    "pipe = Pipeline([('sel. var',selvar),('classif',classif)])\n",
    "\n",
    "# Vérification de l'usage et des propriétés du nouvel objet\n",
    "pipe.fit(X_train,y_train)           # 1. Apprentissage\n",
    "yhat = pipe.predict(X_test)         # 2. inférence\n",
    "tx = accuracy_score(yhat, y_test)   # 3. Evaluation (même si pipe n'entre pas directement en jeu)\n",
    "\n",
    "print(\"taux de bonne classif: \", tx)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plot_tree(classif, filled=True)\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342676e",
   "metadata": {},
   "source": [
    "## C.3. Optimisation de la chaine de traitements\n",
    "\n",
    "La construction n'est pas une fin en soi: l'idée est de pouvoir optimiser la chaine à la fois au niveau des paramètres de pré-traitements et des hyper-paramètres.\n",
    "\n",
    "On veut tester les options suivantes (en combinaison):\n",
    "* Sélection de 2 à 5 variables (avec le sélecteur simple à base de correlation)\n",
    "* SVM\n",
    "    * noyau linéaire\n",
    "    * noyau gaussien avec `gamma = [0.1, 0.5, 1, 2, 5]`\n",
    "    * compromis de régularisation : `C = [0.1,1,5,10,100]`\n",
    "\n",
    "1. Construire la chaine de traitement\n",
    "2. Utiliser le `grid_search` vu dans le TP 1 pour optimiser l'ensemble\n",
    "    * Dans un premier temps, mettre tous les paramètres *en vrac* <BR>\n",
    "    https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "    * Dans un second temps, essayer de ne pas calculer les combinaisons absurdes (tester toutes les valeurs de gamma pour un noyau linéaire) <BR>\n",
    "    Il faut faire des sous-dictionnaires\n",
    "\n",
    "3. Expliquer quelle chaine est retenue et à quel niveau de performances attendu\n",
    "\n",
    "> A votre avis, êtes-vous dans dans un cadre APP/TEST ou dans un cadre APP/VAL/TEST?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la chaine (pipeline)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# V1 : test en vrac (~29 secondes sur mon ordinateur portable)\n",
    "# v2 : test raisonnable (~17 secondes sur mon ordinateur portable)\n",
    "# +> Comprendre pourquoi l'une des approches est plus raisonnable que l'autre\n",
    "\n",
    "### MISE EN PLACE DE LA CHAINE ###\n",
    "estimator = Correl()\n",
    "selector = SequentialFeatureSelector(estimator)\n",
    "classif = svm.SVC()\n",
    "# liste de tuples: titre + objet \n",
    "pipe = Pipeline([('selvar',selector),('classif',classif)])\n",
    "\n",
    "# paramètres en vrac: TITRE__para: [valeurs à tester]\n",
    "# V1\n",
    "grid = {\n",
    "    \"selvar__n_features_to_select\": [2, 3, 4, 5],\n",
    "    \"classif__C\": [0.1,1,5,10,100],\n",
    "    \"classif__gamma\": [0.1, 0.5, 1, 2, 5],\n",
    "    \"classif__kernel\": [\"linear\",\"rbf\"],\n",
    "}\n",
    "#V2 avec sous-dictionnaire \n",
    "grid = [\n",
    "    {\"selvar__n_features_to_select\": [2, 3, 4, 5],\n",
    "    \"classif__C\": [0.1,1,5,10,100],\n",
    "    \"classif__kernel\": [\"linear\"]},\n",
    "    {\"selvar__n_features_to_select\": [2, 3, 4, 5],\n",
    "    \"classif__C\": [0.1,1,5,10,100], \"classif__gamma\": [0.1, 0.5, 1, 2, 5],\n",
    "    \"classif__kernel\": [\"rbf\"]}\n",
    "]\n",
    "# verification des combinaisons qui seront testées\n",
    "# param_grid = list(ParameterGrid(grid))\n",
    "print(list(ParameterGrid(grid)))\n",
    "\n",
    "search = GridSearchCV(pipe, grid, n_jobs=2)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36669a",
   "metadata": {},
   "source": [
    "### Optuna\n",
    "\n",
    "Optuna vous propose des outils avancés pour régler les hyperparamètres:\n",
    "\n",
    "1. Des outils agréables pour le grid-search\n",
    "2. Passer du grid-search à une optimisation fine qui brise la combinatoire\n",
    "\n",
    "J'ai copié les 3 étapes du tuto officiel qui me semble parfait (source: https://optuna.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si besoin\n",
    "# ! pip install optuna\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed0102",
   "metadata": {},
   "source": [
    "#### 1. Se mettre dans les conditions d'un problème d'optimisation\n",
    "\n",
    "Sans optimisation       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a955fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def objective():\n",
    "    iris = sklearn.datasets.load_iris()  # Prepare the data.\n",
    "\n",
    "    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=5, max_depth=3)  # Define the model.\n",
    "\n",
    "    return sklearn.model_selection.cross_val_score(\n",
    "        clf, iris.data, iris.target, n_jobs=-1, cv=3\n",
    "    ).mean()  # Train and evaluate the model.\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(objective()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee1b21",
   "metadata": {},
   "source": [
    "#### 2. Une recherche plus fine pour briser la combinatoire\n",
    "\n",
    "On laisse optuna se *balader* dans l'espace des paramètres\n",
    "\n",
    "1. Remplacer les valeurs de vos paramètres par un générateur:\n",
    "```\n",
    "n_estimators = trial.suggest_int(\"n_estimators\", 2, 20)\n",
    "```\n",
    "2. Lancer l'optimisation\n",
    "```\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 2, 20)\n",
    "    max_depth = int(trial.suggest_float(\"max_depth\", 1, 32, log=True))\n",
    "\n",
    "    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "    return sklearn.model_selection.cross_val_score(\n",
    "        clf, iris.data, iris.target, n_jobs=-1, cv=3\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Accuracy: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9babf0",
   "metadata": {},
   "source": [
    "#### 3. Avec des hyper-paramètres conditionnels aux différents modèles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    classifier = trial.suggest_categorical(\"classifier\", [\"RandomForest\", \"SVC\"])\n",
    "\n",
    "    if classifier == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 2, 20)\n",
    "        max_depth = int(trial.suggest_float(\"max_depth\", 1, 32, log=True))\n",
    "\n",
    "        clf = sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=n_estimators, max_depth=max_depth\n",
    "        )\n",
    "    else:\n",
    "        c = trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True)\n",
    "\n",
    "        clf = sklearn.svm.SVC(C=c, gamma=\"auto\")\n",
    "\n",
    "    return sklearn.model_selection.cross_val_score(\n",
    "        clf, iris.data, iris.target, n_jobs=-1, cv=3\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Accuracy: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85dfbf",
   "metadata": {},
   "source": [
    "## C.4. Exercice\n",
    "\n",
    "Mêmes opérations que précédemment, mais:\n",
    "\n",
    "* Sur les données de qualité du vin\n",
    "* avec une normalisation standard\n",
    "* une random forest ou un XGBoost/catboost dont on fera varier le nombre et la profondeur des arbres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/winequality-red.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "X = data.values[:,:-1]\n",
    "Y = data.values[:,-1]\n",
    "# remise en forme des Y entre 0 et nClasses\n",
    "val = np.unique(Y)\n",
    "transf = dict(zip(val,np.arange(len(val)))) # mapping [x,y,z] => [0,1,2]\n",
    "y = np.vectorize(transf.get)(Y)             # application de la transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87828a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd71d3",
   "metadata": {},
   "source": [
    "## C.5. Chaine de traitement et enrichissement des données\n",
    "\n",
    "Mêmes opérations que précédemment, mais en ajoutant des dimensions.\n",
    "\n",
    "La difficulté est de trouver l'objet scikit-learn qui permet d'insérer des colonnes dans les données\n",
    "\n",
    "* La chaine transformera les variables en catégories (one-hot) \n",
    "* Appliquera une régression logistique\n",
    "\n",
    "Le but est ensuite de chercher à optimiser une chaine de traitement contenant de l'enrichissement de données (par exemple, tester différentes valeurs pour la discrétisation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/winequality-red.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "X = data.values[:,:-1]\n",
    "Y = data.values[:,-1]\n",
    "# remise en forme des Y entre 0 et nClasses\n",
    "val = np.unique(Y)\n",
    "transf = dict(zip(val,np.arange(len(val)))) # mapping [x,y,z] => [0,1,2]\n",
    "y = np.vectorize(transf.get)(Y)             # application de la transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "features_id = [3]\n",
    "scale_id = [1,2]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"untouched\", StandardScaler(), scale_id ), # np.setdiff1d(np.arange(X.shape[1]), features_id)),\n",
    "        (\n",
    "            \"tobin\",\n",
    "            KBinsDiscretizer(n_bins=10, encode='onehot-dense', strategy='uniform'),\n",
    "            features_id,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "#print(np.setdiff1d(np.arange(X.shape[1]), features_id))\n",
    "# print(preprocessor.fit_transform(X).toarray())\n",
    "print(X.shape)\n",
    "\n",
    "log_reg = make_pipeline(preprocessor, LogisticRegression())\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "numeric_features = [\"age\", \"fare\"]\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "categorical_features = [\"embarked\", \"pclass\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            categorical_features,\n",
    "        ),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "log_reg = make_pipeline(preprocessor, SelectKBest(k=7), LogisticRegression())\n",
    "#log_reg.fit(X, y)\n",
    "\n",
    "sc = cross_val_score(log_reg, X, y)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cada42e",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6381aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"4-3-notebook-chaine-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    " \n",
    "f.close()\n",
    "\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
