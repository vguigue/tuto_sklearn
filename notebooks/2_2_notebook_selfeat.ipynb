{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection de caractéristiques\n",
    "\n",
    "Comme nous l'avons vu en cours, les approches de machine-learning ne sont pas réellement capables de sélectionner les bonnes caractéristiques: il est donc essentiel de sélectionner les informations utiles et d'éliminer (au moins une part) de l'information inutile.\n",
    "\n",
    "Ce notebook vise à reproduire l'essentiel des expéricences vues dans les transparents de cours: ce sont les exemples que nous trouvons les plus parlant pour mettre en évidence l'importance de cette tâche critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from outils.frontiere import *\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# données + dimensions inutiles\n",
    "centers = [[-2.0, -2.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 1.5]\n",
    "X, y = make_blobs(n_samples=100, centers=centers, cluster_std=clusters_std,  n_features=2,   random_state=0) # 100 pts, 2classes, 2dim \n",
    "\n",
    "# ajout de bruit\n",
    "ndim_noise = 20\n",
    "Noise = np.random.randn(len(X), ndim_noise)*clusters_std[0]\n",
    "Xn = np.concatenate((X,Noise), axis=1)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des données complètes\n",
    "\n",
    "ind = np.argsort(y_train)\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(X_train[ind])\n",
    "plt.savefig(\"fig/Xbruit.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Curse of dimensionality / fléau de la dimensionalité\n",
    "\n",
    "1. Vérifier la dimension des données générées ci-dessus\n",
    "2. Vérifier que vous pouvez afficher ce problème jouet en utilisant les deux premières dimension de `X_train`\n",
    "3. Reproduction de l'expérience sur le fléau de la dimensionalité\n",
    "\n",
    "<img src = \"fig/curse.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension de X\n",
    "\n",
    "# Scatter plot de X (deux première dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduction de l'expérience sur le fléau de la dimensionalité\n",
    "mod = svm.SVC(gamma = 0.1) # proposition de classifieur qui doit mettre en évidence le phénomène\n",
    "perf_train = []\n",
    "perf_test  = []\n",
    "\n",
    "for ndim in range(0,X_train.shape[1],4) : # Ajout des dimensions de bruit 4 par 4\n",
    "    # réduction des données\n",
    "    Xtmp_train = \n",
    "    Xtmp_test  = \n",
    "    # apprentissage du modèle \n",
    "\n",
    "    # évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Réduction / sélection de dimensions\n",
    "\n",
    "### B.1. Stratégies implémentées dans scikit-learn\n",
    "\n",
    "Nous allons voir quelques stratégies de réduction de la dimensionnalité:\n",
    "\n",
    "* Expérience naïve de corrélation entre les variables et $y$\n",
    "* Elimination successive des dimensions les moins intéressantes (ou ajout des dimensions intéressante)\n",
    "    * doc: [lien](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)\n",
    "    * Stratégie potentiellement très couteuse: il faut bien regarder les paramètres\n",
    "        * Que penser de la différence entre *forward* et *backward*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de correlation entre les variables et y\n",
    "\n",
    "corr = np.abs(X_train.T @ y_train)\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(corr)), corr)\n",
    "\n",
    "# ... Sur un exemple jouet, ça marche vraiment très bien ...\n",
    "# A tester sur des exemples réels, on ne sait jamais, le critère est intéressant !\n",
    "\n",
    "plt.savefig(\"fig/corr_var.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "estimator = svm.SVC(kernel=\"linear\") # je choisis d'utiliser le score en classif d'un SVM lineaire\n",
    "selector = SequentialFeatureSelector(estimator, n_features_to_select=2)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "print(selector.get_support())\n",
    "\n",
    "# il eset ensuite possible de filtrer les données:\n",
    "\n",
    "Xnew = selector.transform(X_train)\n",
    "print(\"Ancienne dimensions : \",X_train.shape)\n",
    "print(\"Nouvelles dimensions : \",Xnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact en temps de calcul des options:\n",
    "import time\n",
    "\n",
    "# forward\n",
    "t = time.time()\n",
    "estimator = svm.SVC(kernel=\"linear\")\n",
    "selector = SequentialFeatureSelector(estimator, n_features_to_select=2)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "print(selector.get_support())\n",
    "print(\"durée: {}\".format(time.time()-t))\n",
    "\n",
    "# backward\n",
    "# Ajouter la bonne option et lancer le chronomètre. \n",
    "# bien comprendre l'algorithme qui tourne derrière\n",
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2. ACP/PCA : Analyse en composantes principales\n",
    "\n",
    "Trouver les combinaisons de variables qui permettent *d'expliquer les données*\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "* Ce critère, très efficace et très utilisé, n'est pas lié aux étiquettes $y$!\n",
    "* dans ce critère, on peut analyser les valeurs propres pour choisir combien d'axes conserver\n",
    "\n",
    "<img src=\"fig/valp.png\">\n",
    "\n",
    "* D'après la figure, une valeur propre (ou deux) sont suffisantes pour bien représenter les données...\n",
    "    * Projeter les données dans cet espace\n",
    "    * apprendre un classifieur et comparer les performances par rapport à l'espace de représentation d'origine\n",
    "\n",
    "<img src=\"fig/pca.png\">\n",
    "\n",
    "**Note:** Les performances sur 1 seule dimension sont optimale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# application de la PCA\n",
    "\n",
    "pca = PCA() # on peut préciser le nombre de valeurs propres à conserver /calculer: n_components=2\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Analyse des valeurs propres:\n",
    "plt.figure(figsize=[8,4])\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(X_train.shape[1]), pca.singular_values_)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.grid()\n",
    "#plt.savefig('fig/valp.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection des données dans cet espace à un ou deux axes:\n",
    "# + apprentissage de modele\n",
    "# + evaluation des performances\n",
    "\n",
    "Xnew = pca.transform(X_train) # projection sur tous les axes\n",
    "XnewT = pca.transform(X_test) # projection sur tous les axes\n",
    "\n",
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de nuage de point projeté en 1D et 2D\n",
    "\n",
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peut-on utiliser la PCA sur les données USPS avec lesquelles nous avons joué il y a quelques semaines??\n",
    "\n",
    "OUI! Evidemment.\n",
    "\n",
    "* Idée 1: on va visualiser les données USPS en 2 dimensions!\n",
    "* Idée 2: on va voir s'il est possible de classer les données USPS dans cet espace réduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "import pickle as pkl\n",
    "data = pkl.load(open(\"data/usps.pkl\",'rb')) \n",
    "# data est un dictionnaire contenant les champs explicites X_train, X_test, Y_train, Y_test\n",
    "X_train = np.array(data[\"X_train\"],dtype=float) # changement de type pour éviter les problèmes d'affichage\n",
    "X_test = np.array(data[\"X_test\"],dtype=float)\n",
    "Y_train = data[\"Y_train\"]\n",
    "Y_test = data[\"Y_test\"]\n",
    "\n",
    "# pour rappel sur la structuration des données: affichage de l'image 18 avec reshape\n",
    "plt.figure()\n",
    "plt.imshow(X_train[18].reshape(16,16),cmap=\"gray\")\n",
    "plt.title(\"Image de : {}\".format(Y_train[18]))\n",
    "\n",
    "# Faire la projection\n",
    "\n",
    "# Affichage de la base projetée sur les deux premiers axes\n",
    "\n",
    "# Afficher également les axes de projection eux-mêmes\n",
    "\n",
    "# Calculer la performance en fonction du nombre d'axes utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3. Régularisation\n",
    "\n",
    "### B.3.1. Régularisation l2\n",
    "\n",
    "Nous avons déjà croisé la régularisation L2 avec les SVM. Essayons de pousser un peu plus loin l'analyse de ce type d'approches:\n",
    "\n",
    "$$\\text{Formulation Ridge: }\\qquad \\mathcal L = \\sum_{i=1}^n \\left( \\sum_{j=1}^d w_j x_{ij} - y_i\\right)^2 + \\textcolor{red}{C} \\|w\\|^2$$\n",
    "\n",
    "Après la phase de génération de données, nous allons faire varier $C$ et étudier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regénération des données bruitées\n",
    "\n",
    "# données + dimensions inutiles\n",
    "centers = [[-2.0, -2.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 1.5]\n",
    "X, y = make_blobs(n_samples=100, centers=centers, cluster_std=clusters_std,  n_features=2,   random_state=0) # 100 pts, 2classes, 2dim \n",
    "\n",
    "# ajout de bruit\n",
    "ndim_noise = 20\n",
    "Noise = np.random.randn(len(X), ndim_noise)*clusters_std[0]\n",
    "Xn = np.concatenate((X,Noise), axis=1)\n",
    "\n",
    "\n",
    "# split (données classique + données bruitées)\n",
    "Xn_train, Xn_test, yn_train, yn_test = train_test_split(Xn, y, test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# différentes valeurs de C sur les données d'origine\n",
    "\n",
    "all_a = 10**(np.linspace(10,-10,11))\n",
    "p_a = [] # perf en apprentissage\n",
    "p_t = [] # perf en test\n",
    "wc = []  # cardinal des coefficients non nul\n",
    "\n",
    "\n",
    "for a in all_a:\n",
    "    mod = RidgeClassifier(alpha=a)\n",
    "    mod.fit(Xn_train,yn_train)\n",
    "    # Compléter l'évaluation et le comptage des coefficients non nuls\n",
    "    ###   TODO  ###\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(p_a)\n",
    "plt.plot(p_t)\n",
    "plt.plot(wc)\n",
    "plt.grid()\n",
    "plt.xlabel('Régularisation (décroissante)')\n",
    "plt.ylabel('Perf / Nombre dim')\n",
    "plt.legend(['P train', 'P test', '#w'])\n",
    "\n",
    "# plt.savefig('fig/reg_L2.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3.2. Régularisation l1\n",
    "\n",
    "Introduction d'une régularisation parcimonieuse\n",
    "\n",
    "$$\\text{Formulation LASSO: }\\qquad \\mathcal L = \\sum_{i=1}^n \\left( \\sum_{j=1}^d w_j x_{ij} - y_i\\right)^2 + \\textcolor{red}{C} \\sum_{j=1}^d |w_j|$$\n",
    "\n",
    "Après la phase de génération de données, nous allons faire varier $C$ et étudier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# différentes valeurs de C sur les données d'origine (fait automatiquement ici)\n",
    "alphas,  coef_ ,active = lasso_path(Xn_train,yn_train) # fit automatique à l'intérieur\n",
    "# note: le modèle LASSO marche comme les autres modèles\n",
    "# pour gagner du temps, j'ai juste calculer tous les modèles d'un coup :)\n",
    "\n",
    "p_a = []\n",
    "p_t = []\n",
    "wc = []\n",
    "\n",
    "for i in range(coef_.shape[1]):\n",
    "    \n",
    "    yhata = np.where(Xn_train.dot(coef_[:,i]) > 0.5, 1, 0)\n",
    "    yhatt = np.where(Xn_test.dot(coef_[:,i]) > 0.5, 1, 0)\n",
    "    p_a.append(accuracy_score(yhata, yn_train))\n",
    "    p_t.append(accuracy_score(yhatt, yn_test))\n",
    "    wc.append(np.where(np.abs(coef_[:,i])>1e-5, 1, 0).mean())\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(p_a)\n",
    "plt.plot(p_t)\n",
    "plt.plot(wc)\n",
    "plt.grid()\n",
    "plt.xlabel('Régularisation (décroissante)')\n",
    "plt.ylabel('Perf / Nombre dim')\n",
    "plt.legend(['P train', 'P test', '#w'])\n",
    "\n",
    "plt.savefig('fig/reg_L1.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3.3. Régularisation Elastic Net\n",
    "\n",
    "Combinaison de deux régularisations L2 et L1. L'idée est de jouer principalement sur la L2 et de mettre un peu de L1 pour la sparsité.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# différentes valeurs de C sur les données d'origine\n",
    "\n",
    "all_a = (np.linspace(2.5,0.1,20))\n",
    "p_a = [] # perf en apprentissage\n",
    "p_t = [] # perf en test\n",
    "wc = []  # cardinal des coefficients non nul\n",
    "\n",
    "for a in all_a:\n",
    "    mod = ElasticNet(alpha=a, l1_ratio=0.5)\n",
    "    mod.fit(Xn_train,yn_train)\n",
    "    # Compléter l'évaluation et le comptage des coefficients non nuls\n",
    "    # ATTENTION: par défaut, il s'agit d'un modèle de régression: \n",
    "    # il faut remettre les classes en face des classes pour utiliser les métriques\n",
    "    ###   TODO  ###\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(p_a)\n",
    "plt.plot(p_t)\n",
    "plt.plot(wc, '+-')\n",
    "plt.grid()\n",
    "plt.xlabel('Régularisation (décroissante)')\n",
    "plt.ylabel('Perf / Nombre dim')\n",
    "plt.legend(['P train', 'P test', '#w'])\n",
    "\n",
    "#plt.savefig('fig/reg_L1L2.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
